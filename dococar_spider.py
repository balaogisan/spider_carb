#!/usr/bin/env python3
"""
DoCoCAR - å°åŒ— 119 å®¶æ±½è»Šéè†œåº—  (å«é›»è©±)  â†’ dococar_taipei_detailing.xlsx
"""
import re, time, random
from pathlib import Path
import pandas as pd
import cloudscraper                       # â† è‡ªå‹•è§£ Cloudflare
from bs4 import BeautifulSoup
from tqdm import tqdm
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

BASE = "https://dococar.com"
LIST_URL = f"{BASE}/coating/taipei"
HEADERS = {"User-Agent":
           "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
           "AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36"}
scraper = cloudscraper.create_scraper(browser={"browser": "chrome",
                                               "platform": "darwin",
                                               "mobile": False})

def get_soup(url):
    try:
        response = scraper.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        return BeautifulSoup(response.text, "lxml")      # æ²’è£ lxml å°±æ”¹ "html.parser"
    except Exception as e:
        logger.error(f"Error fetching {url}: {e}")
        raise

# ---------- STEP 1ï¼šæŠ“ 12 å€‹åˆ†é  ----------
print("STEP 1  å–å¾—è¡Œæ”¿å€åˆ†é â€¦")
try:
    district_links = [f"{BASE}/coating/taipei/{i}" for i in range(1, 13)]
    print("  â–¶ ç”Ÿæˆ 12 å€‹åˆ†é é€£çµ â‡’", district_links)
    
except Exception as e:
    logger.error(f"Error in STEP 1: {e}")
    print(f"  âŒ STEP 1 å¤±æ•—: {e}")
    exit(1)

# ---------- STEP 2ï¼šåˆ—è¡¨æ“·å–åº—å®¶å¡ç‰‡ ----------
print("STEP 2  æ“·å–åº—å®¶åˆ—è¡¨â€¦")
stores = []
for i, link in enumerate(district_links, 1):
    try:
        print(f"  è™•ç†åˆ†é  {i}/{len(district_links)}: {link}")
        d_soup = get_soup(link)
        
        # More robust district extraction
        h1_element = d_soup.select_one("h1")
        if h1_element:
            district = h1_element.text.split("å°åŒ—å¸‚")[-1].split("æ±½")[0]
        else:
            district = f"æœªçŸ¥å€åŸŸ{i}"
            logger.warning(f"Could not extract district from {link}")
        
        # Extract store cards
        cards = d_soup.select("a[href^='/detailing/']")
        for card in cards:
            store_name = card.get_text(strip=True)
            if store_name:  # Only add if store name exists
                stores.append({
                    "store_name": store_name,
                    "district":   district,
                    "detail_url": BASE + card["href"]
                })
        
        print(f"    â–¶ æ‰¾åˆ° {len(cards)} å®¶åº—")
        time.sleep(random.uniform(0.5, 1.0))  # Rate limiting
        
    except Exception as e:
        logger.error(f"Error processing district page {link}: {e}")
        continue

print("  â–¶ å¾åˆ—è¡¨æŠ“åˆ°", len(stores), "é–“åº—")

# ---------- STEP 3ï¼šé€åº—æ“·å–é›»è©± / è©•åˆ† ----------
phone_re = re.compile(r"\d{2,4}-?\d{3,4}-?\d{3,4}")
def parse_detail(url):
    try:
        s = get_soup(url)
        
        # Extract phone number
        phone_m = phone_re.search(s.get_text())
        phone = phone_m.group(0) if phone_m else ""
        
        # Extract rating - try multiple selectors
        rating_selectors = ['[itemprop="ratingValue"]', '.rating-value', '.star-rating']
        rating = ""
        for selector in rating_selectors:
            rating_elem = s.select_one(selector)
            if rating_elem:
                rating = rating_elem.text.strip()
                break
        
        # Extract reviews count - try multiple selectors  
        review_selectors = ['[itemprop="ratingCount"]', '.review-count', '.reviews-count']
        reviews = ""
        for selector in review_selectors:
            reviews_elem = s.select_one(selector)
            if reviews_elem:
                reviews = reviews_elem.text.strip()
                break
        
        # Extract address - try multiple selectors
        address_selectors = ['[itemprop="address"]', '.address', '.location']
        address = ""
        for selector in address_selectors:
            address_elem = s.select_one(selector)
            if address_elem:
                address = address_elem.text.strip()
                break
        
        return phone, rating, reviews, address
        
    except Exception as e:
        logger.error(f"Error parsing detail page {url}: {e}")
        return "", "", "", ""

print("STEP 3  æ“·å–è©³ç´°é  (ç´„ 60 ç§’)â€¦")
if not stores:
    print("  âš ï¸  æ²’æœ‰åº—å®¶è³‡æ–™ï¼Œè·³éè©³ç´°é æ“·å–")
else:
    for row in tqdm(stores, ncols=80, desc="æ“·å–åº—å®¶è©³ç´°è³‡æ–™"):
        try:
            phone, rating, reviews, addr = parse_detail(row["detail_url"])
            row.update({"phone": phone, "rating": rating,
                        "reviews": reviews, "address": addr})
        except Exception as e:
            logger.error(f"Error processing store {row.get('store_name', 'Unknown')}: {e}")
            row.update({"phone": "", "rating": "", "reviews": "", "address": ""})
        
        # More random sleep to avoid being blocked
        time.sleep(random.uniform(0.8, 1.5))

# ---------- STEP 4ï¼šè¼¸å‡º ----------
if stores:
    df = pd.DataFrame(stores)
    out = Path("dococar_taipei_detailing.xlsx")
    
    # Ensure output directory exists
    out.parent.mkdir(parents=True, exist_ok=True)
    
    # Save to Excel
    try:
        df.to_excel(out, index=False)
        print(f"âœ… å®Œæˆï¼š{len(df)} ç­†  â†’  {out.resolve()}")
        
        # Show summary statistics
        print("\nğŸ“Š è³‡æ–™æ‘˜è¦:")
        print(f"  - ç¸½åº—å®¶æ•¸: {len(df)}")
        print(f"  - æœ‰é›»è©±è™Ÿç¢¼: {len(df[df['phone'] != ''])}")
        print(f"  - æœ‰è©•åˆ†: {len(df[df['rating'] != ''])}")
        print(f"  - æœ‰åœ°å€: {len(df[df['address'] != ''])}")
        print(f"  - è¡Œæ”¿å€åˆ†å¸ƒ: {df['district'].value_counts().to_dict()}")
        
    except Exception as e:
        logger.error(f"Error saving to Excel: {e}")
        print(f"âŒ å„²å­˜å¤±æ•—: {e}")
        
        # Try to save as CSV as fallback
        try:
            csv_out = out.with_suffix('.csv')
            df.to_csv(csv_out, index=False)
            print(f"ğŸ”„ å·²æ”¹å­˜ç‚º CSV: {csv_out.resolve()}")
        except Exception as csv_e:
            logger.error(f"Error saving CSV: {csv_e}")
            print(f"âŒ CSV å„²å­˜ä¹Ÿå¤±æ•—: {csv_e}")
else:
    print("âŒ æ²’æœ‰è³‡æ–™å¯è¼¸å‡º")
